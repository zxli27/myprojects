{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SENG474_Word2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1unfxqKkm7ZCFTN-3zEBj-Ge6oA4Wo6GD",
      "authorship_tag": "ABX9TyPMocYL8aQi4WEMzebJA7VY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zt55699/IMDB-Sentiment/blob/main/SENG474_Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4nA0J-lrUOF"
      },
      "source": [
        "# Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Foc-VvDWgttf",
        "outputId": "c4839eea-0034-49d5-944e-51590496a532"
      },
      "source": [
        "%rm -rf IMDB-Sentiment\n",
        "!git clone https://github.com/zt55699/IMDB-Sentiment.git\n",
        "%cd IMDB-Sentiment/\n",
        "%ls"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'IMDB-Sentiment'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 8 (delta 1), reused 8 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (8/8), done.\n",
            "/content/IMDB-Sentiment/IMDB-Sentiment/IMDB-Sentiment\n",
            "labeledTrainData.tsv  README.md  testData.tsv  unlabeledTrainData.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7K9BddCnqVz"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read data from files \n",
        "train_data = pd.read_csv( \"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
        "test_data = pd.read_csv( \"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
        "unlabeled_train = pd.read_csv( \"unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1o8MWtDqUnF",
        "outputId": "51f26868-b5b0-40af-af97-047d9c25fdac"
      },
      "source": [
        "print(train_data.head)\n",
        "print(test_data.head)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method NDFrame.head of               id  sentiment                                             review\n",
            "0       \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
            "1       \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
            "2       \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
            "3       \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
            "4       \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ...\n",
            "...          ...        ...                                                ...\n",
            "24995   \"3453_3\"          0  \"It seems like more consideration has gone int...\n",
            "24996   \"5064_1\"          0  \"I don't believe they made this film. Complete...\n",
            "24997  \"10905_3\"          0  \"Guy is a loser. Can't get girls, needs to bui...\n",
            "24998  \"10194_3\"          0  \"This 30 minute documentary BuÃ±uel made in the...\n",
            "24999   \"8478_8\"          1  \"I saw this movie as a child and it broke my h...\n",
            "\n",
            "[25000 rows x 3 columns]>\n",
            "<bound method NDFrame.head of                id                                             review\n",
            "0      \"12311_10\"  \"Naturally in a film who's main themes are of ...\n",
            "1        \"8348_2\"  \"This movie is a disaster within a disaster fi...\n",
            "2        \"5828_4\"  \"All in all, this is a movie for kids. We saw ...\n",
            "3        \"7186_2\"  \"Afraid of the Dark left me with the impressio...\n",
            "4       \"12128_7\"  \"A very accurate depiction of small time mob l...\n",
            "...           ...                                                ...\n",
            "24995   \"2155_10\"  \"Sony Pictures Classics, I'm looking at you! S...\n",
            "24996     \"59_10\"  \"I always felt that Ms. Merkerson had never go...\n",
            "24997    \"2531_1\"  \"I was so disappointed in this movie. I am ver...\n",
            "24998    \"7772_8\"  \"From the opening sequence, filled with black ...\n",
            "24999  \"11465_10\"  \"This is a great horror film for people who do...\n",
            "\n",
            "[25000 rows x 2 columns]>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JG1KtGzrNKN"
      },
      "source": [
        "# Data Cleaning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNJi3tXkjrLO"
      },
      "source": [
        "Gensim preprocessing doc: https://radimrehurek.com/gensim/parsing/preprocessing.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WsvEJ3GtSL6",
        "outputId": "6bd2faa2-144d-4652-ae29-e133134e517d"
      },
      "source": [
        "import gensim.parsing.preprocessing as gp\n",
        "from gensim.parsing.preprocessing import preprocess_string\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "\n",
        "# Cast words to lower case; remove HTML tags, puctuation, numbers, short words and meaningless stopwords\n",
        "# Use Porter Stemming e.g. treat \"go\", \"going\", and \"went\" as the same word\n",
        "# Not remove stop words here because Word2Vec relies on the broader context of the sentence\n",
        "FILTERS = [lambda x: x.lower(), gp.strip_tags, gp.strip_punctuation, \n",
        "           gp.strip_multiple_whitespaces, gp.strip_short, gp.stem_text, \n",
        "           gp.remove_stopwords, gp.strip_numeric] # maybe not remove number as well\n",
        "\n",
        "# clean a sentence, return a list of words\n",
        "def clean_sentence(raw_sentence):\n",
        "  return preprocess_string(raw_sentence, FILTERS)\n",
        "\n",
        "r1 = train_data[\"review\"][0]\n",
        "print(\"Before: \", r1)\n",
        "print(\"After: \", clean_sentence(r1))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before:  \"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n",
            "After:  ['with', 'all', 'thi', 'stuff', 'go', 'down', 'the', 'moment', 'with', 'start', 'listen', 'hi', 'music', 'watch', 'the', 'odd', 'documentari', 'here', 'and', 'there', 'watch', 'the', 'wiz', 'and', 'watch', 'moonwalk', 'again', 'mayb', 'just', 'want', 'get', 'certain', 'insight', 'into', 'thi', 'gui', 'who', 'thought', 'wa', 'realli', 'cool', 'the', 'eighti', 'just', 'mayb', 'make', 'mind', 'whether', 'guilti', 'innoc', 'moonwalk', 'part', 'biographi', 'part', 'featur', 'film', 'which', 'rememb', 'go', 'see', 'the', 'cinema', 'when', 'wa', 'origin', 'releas', 'some', 'ha', 'subtl', 'messag', 'about', 'feel', 'toward', 'the', 'press', 'and', 'also', 'the', 'obviou', 'messag', 'drug', 'ar', 'bad', 'kai', 'visual', 'impress', 'but', 'cours', 'thi', 'all', 'about', 'michael', 'jackson', 'unless', 'you', 'remot', 'like', 'anywai', 'then', 'you', 'ar', 'go', 'hate', 'thi', 'and', 'find', 'bore', 'some', 'mai', 'call', 'egotist', 'for', 'consent', 'the', 'make', 'thi', 'movi', 'but', 'and', 'most', 'hi', 'fan', 'would', 'sai', 'that', 'made', 'for', 'the', 'fan', 'which', 'true', 'realli', 'nice', 'him', 'the', 'actual', 'featur', 'film', 'bit', 'when', 'final', 'start', 'onli', 'for', 'minut', 'exclud', 'the', 'smooth', 'crimin', 'sequenc', 'and', 'joe', 'pesci', 'convinc', 'psychopath', 'all', 'power', 'drug', 'lord', 'why', 'want', 'dead', 'bad', 'beyond', 'becaus', 'overheard', 'hi', 'plan', 'nah', 'joe', 'pesci', 'charact', 'rant', 'that', 'want', 'peopl', 'know', 'who', 'suppli', 'drug', 'etc', 'dunno', 'mayb', 'just', 'hate', 'music', 'lot', 'cool', 'thing', 'thi', 'like', 'turn', 'into', 'car', 'and', 'robot', 'and', 'the', 'whole', 'speed', 'demon', 'sequenc', 'also', 'the', 'director', 'must', 'have', 'had', 'the', 'patienc', 'saint', 'when', 'came', 'film', 'the', 'kiddi', 'bad', 'sequenc', 'usual', 'director', 'hate', 'work', 'with', 'on', 'kid', 'let', 'alon', 'whole', 'bunch', 'them', 'perform', 'complex', 'danc', 'scene', 'bottom', 'line', 'thi', 'movi', 'for', 'peopl', 'who', 'like', 'on', 'level', 'anoth', 'which', 'think', 'most', 'peopl', 'not', 'then', 'stai', 'awai', 'doe', 'try', 'and', 'give', 'off', 'wholesom', 'messag', 'and', 'iron', 'bestest', 'buddi', 'thi', 'movi', 'girl', 'michael', 'jackson', 'truli', 'on', 'the', 'most', 'talent', 'peopl', 'ever', 'grace', 'thi', 'planet', 'but', 'guilti', 'well', 'with', 'all', 'the', 'attent', 'gave', 'thi', 'subject', 'hmmm', 'well', 'don', 'know', 'becaus', 'peopl', 'can', 'differ', 'behind', 'close', 'door', 'know', 'thi', 'for', 'fact', 'either', 'extrem', 'nice', 'but', 'stupid', 'gui', 'on', 'the', 'most', 'sickest', 'liar', 'hope', 'not', 'the', 'latter']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnEAyrFJYWVF"
      },
      "source": [
        "# Data Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDRO7ImsX8Dq"
      },
      "source": [
        "Word2Vec expects single sentences as inputs, each one as a list of words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4I1FY_za7DE",
        "outputId": "13572bd3-509c-407a-d50e-b7e5033a546c"
      },
      "source": [
        "from gensim.summarization.textcleaner import split_sentences\n",
        "\n",
        "# split a review by sentences, return a list of sentences, for each is a list of words\n",
        "def split_review (raw_review):\n",
        "  raw_sentences = split_sentences(raw_review)\n",
        "  clean_sentences = []\n",
        "  for s in raw_sentences:\n",
        "    if len(s) > 0:\n",
        "      clean_sentences.append( clean_sentence(s))\n",
        "  return clean_sentences\n",
        "\n",
        "print(\"Before: \", r1)\n",
        "print(\"After: \", split_review(r1))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before:  \"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n",
            "After:  [['with', 'all', 'thi', 'stuff', 'go', 'down', 'the', 'moment', 'with', 'start', 'listen', 'hi', 'music', 'watch', 'the', 'odd', 'documentari', 'here', 'and', 'there', 'watch', 'the', 'wiz', 'and', 'watch', 'moonwalk', 'again'], ['mayb', 'just', 'want', 'get', 'certain', 'insight', 'into', 'thi', 'gui', 'who', 'thought', 'wa', 'realli', 'cool', 'the', 'eighti', 'just', 'mayb', 'make', 'mind', 'whether', 'guilti', 'innoc'], ['moonwalk', 'part', 'biographi', 'part', 'featur', 'film', 'which', 'rememb', 'go', 'see', 'the', 'cinema', 'when', 'wa', 'origin', 'releas'], ['some', 'ha', 'subtl', 'messag', 'about', 'feel', 'toward', 'the', 'press', 'and', 'also', 'the', 'obviou', 'messag', 'drug', 'ar', 'bad', 'kai', 'visual', 'impress', 'but', 'cours', 'thi', 'all', 'about', 'michael', 'jackson', 'unless', 'you', 'remot', 'like', 'anywai', 'then', 'you', 'ar', 'go', 'hate', 'thi', 'and', 'find', 'bore'], ['some', 'mai', 'call', 'egotist', 'for', 'consent', 'the', 'make', 'thi', 'movi', 'but', 'and', 'most', 'hi', 'fan', 'would', 'sai', 'that', 'made', 'for', 'the', 'fan', 'which', 'true', 'realli', 'nice', 'him', 'the', 'actual', 'featur', 'film', 'bit', 'when', 'final', 'start', 'onli', 'for', 'minut', 'exclud', 'the', 'smooth', 'crimin', 'sequenc', 'and', 'joe', 'pesci', 'convinc', 'psychopath', 'all', 'power', 'drug', 'lord'], ['why', 'want', 'dead', 'bad', 'beyond'], ['becaus', 'overheard', 'hi', 'plan'], ['nah', 'joe', 'pesci', 'charact', 'rant', 'that', 'want', 'peopl', 'know', 'who', 'suppli', 'drug', 'etc', 'dunno', 'mayb', 'just', 'hate', 'music', 'lot', 'cool', 'thing', 'thi', 'like', 'turn', 'into', 'car', 'and', 'robot', 'and', 'the', 'whole', 'speed', 'demon', 'sequenc'], ['also', 'the', 'director', 'must', 'have', 'had', 'the', 'patienc', 'saint', 'when', 'came', 'film', 'the', 'kiddi', 'bad', 'sequenc', 'usual', 'director', 'hate', 'work', 'with', 'on', 'kid', 'let', 'alon', 'whole', 'bunch', 'them', 'perform', 'complex', 'danc', 'scene', 'bottom', 'line', 'thi', 'movi', 'for', 'peopl', 'who', 'like', 'on', 'level', 'anoth', 'which', 'think', 'most', 'peopl'], ['not', 'then', 'stai', 'awai'], ['doe', 'try', 'and', 'give', 'off', 'wholesom', 'messag', 'and', 'iron', 'bestest', 'buddi', 'thi', 'movi', 'girl'], ['michael', 'jackson', 'truli', 'on', 'the', 'most', 'talent', 'peopl', 'ever', 'grace', 'thi', 'planet', 'but', 'guilti'], ['well', 'with', 'all', 'the', 'attent', 'gave', 'thi', 'subject', 'hmmm', 'well', 'don', 'know', 'becaus', 'peopl', 'can', 'differ', 'behind', 'close', 'door', 'know', 'thi', 'for', 'fact'], ['either', 'extrem', 'nice', 'but', 'stupid', 'gui', 'on', 'the', 'most', 'sickest', 'liar'], ['hope', 'not', 'the', 'latter']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSg3RlZEZyqu",
        "outputId": "b2821a20-1980-417b-ed34-0c6a00df5a56"
      },
      "source": [
        "# prepare input data for Word2Vec (takes couple minutes):\n",
        "all_sentences = []  \n",
        "\n",
        "print(f'Parsing {len(train_data[\"review\"])} sentences from training set...')\n",
        "train_size = len(train_data[\"review\"])\n",
        "for i in range (0, train_size):\n",
        "    # report progress\n",
        "    progress = (i+1)/train_size *100\n",
        "    if( progress%20 == 0 ):\n",
        "        print(f'   {progress}%')  \n",
        "    all_sentences += split_review( train_data[\"review\"][i])\n",
        "\n",
        "print(f'Parsing {len(unlabeled_train[\"review\"])} sentences from unlabeled set...')\n",
        "unlabel_size = len(unlabeled_train[\"review\"])\n",
        "for i in range (0, unlabel_size):\n",
        "    # report progress\n",
        "    progress = (i+1)/unlabel_size *100\n",
        "    if( progress%20 == 0 ):\n",
        "        print(f'   {progress}%')  \n",
        "    all_sentences += split_review(unlabeled_train[\"review\"][i])"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsing 25000 sentences from training set...\n",
            "   20.0%\n",
            "   40.0%\n",
            "   60.0%\n",
            "   80.0%\n",
            "   100.0%\n",
            "Parsing 50000 sentences from unlabeled set...\n",
            "   20.0%\n",
            "   40.0%\n",
            "   60.0%\n",
            "   80.0%\n",
            "   100.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxGEFvNrkZZC",
        "outputId": "6773f374-54e3-4b72-843a-2b4a7f8cf52a"
      },
      "source": [
        "print(\"Total:\", len(all_sentences), \"sentences\")\n",
        "print(all_sentences[0])"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total: 792761 sentences\n",
            "['with', 'all', 'thi', 'stuff', 'go', 'down', 'the', 'moment', 'with', 'start', 'listen', 'hi', 'music', 'watch', 'the', 'odd', 'documentari', 'here', 'and', 'there', 'watch', 'the', 'wiz', 'and', 'watch', 'moonwalk', 'again']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15U8kEvAlvIw"
      },
      "source": [
        "# Word2Vec Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJFEnbCJmbQf",
        "outputId": "9e17ecf9-8207-43d5-dd76-78b1249c2cea"
      },
      "source": [
        "# Output messages for training\n",
        "from gensim.models import word2vec\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(\n",
        "    format='%(asctime)s [%(levelname)s] %(name)s - %(message)s',\n",
        "    level=logging.INFO,\n",
        "    datefmt='%Y-%m-%d %H:%M:%S',\n",
        "    stream=sys.stdout,\n",
        ")\n",
        "log = logging.getLogger('notebook')\n",
        "\n",
        "# parameters\n",
        "num_features = 300    # Word vector dimensionality                      \n",
        "min_word_count = 40   # Minimum word count                        \n",
        "num_workers = 4       # Number of threads to run in parallel\n",
        "context = 10          # Context window size                                                                                    \n",
        "downsampling = 1e-3   # Downsample setting for frequent words\n",
        "\n",
        "# model training\n",
        "model = word2vec.Word2Vec(all_sentences, workers=num_workers, \n",
        "            size=num_features, min_count = min_word_count, \n",
        "            window = context, sample = downsampling)\n",
        "\n",
        "model.init_sims(replace=True) # internally calculates unit-length normalized vectors"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model...\n",
            "2021-03-11 09:48:39 [INFO] gensim.models.word2vec - collecting all words and their counts\n",
            "2021-03-11 09:48:39 [INFO] gensim.models.word2vec - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2021-03-11 09:48:39 [INFO] gensim.models.word2vec - PROGRESS: at sentence #10000, processed 176663 words, keeping 12579 word types\n",
            "2021-03-11 09:48:39 [INFO] gensim.models.word2vec - PROGRESS: at sentence #20000, processed 353909 words, keeping 17283 word types\n",
            "2021-03-11 09:48:39 [INFO] gensim.models.word2vec - PROGRESS: at sentence #30000, processed 526263 words, keeping 20712 word types\n",
            "2021-03-11 09:48:39 [INFO] gensim.models.word2vec - PROGRESS: at sentence #40000, processed 703043 words, keeping 23488 word types\n",
            "2021-03-11 09:48:39 [INFO] gensim.models.word2vec - PROGRESS: at sentence #50000, processed 877247 words, keeping 25849 word types\n",
            "2021-03-11 09:48:40 [INFO] gensim.models.word2vec - PROGRESS: at sentence #60000, processed 1049749 words, keeping 27842 word types\n",
            "2021-03-11 09:48:40 [INFO] gensim.models.word2vec - PROGRESS: at sentence #70000, processed 1223215 words, keeping 29600 word types\n",
            "2021-03-11 09:48:40 [INFO] gensim.models.word2vec - PROGRESS: at sentence #80000, processed 1393041 words, keeping 31247 word types\n",
            "2021-03-11 09:48:40 [INFO] gensim.models.word2vec - PROGRESS: at sentence #90000, processed 1567910 words, keeping 32930 word types\n",
            "2021-03-11 09:48:40 [INFO] gensim.models.word2vec - PROGRESS: at sentence #100000, processed 1740943 words, keeping 34318 word types\n",
            "2021-03-11 09:48:40 [INFO] gensim.models.word2vec - PROGRESS: at sentence #110000, processed 1914437 words, keeping 35631 word types\n",
            "2021-03-11 09:48:40 [INFO] gensim.models.word2vec - PROGRESS: at sentence #120000, processed 2087278 words, keeping 37027 word types\n",
            "2021-03-11 09:48:40 [INFO] gensim.models.word2vec - PROGRESS: at sentence #130000, processed 2258452 words, keeping 38239 word types\n",
            "2021-03-11 09:48:40 [INFO] gensim.models.word2vec - PROGRESS: at sentence #140000, processed 2431894 words, keeping 39407 word types\n",
            "2021-03-11 09:48:40 [INFO] gensim.models.word2vec - PROGRESS: at sentence #150000, processed 2605882 words, keeping 40586 word types\n",
            "2021-03-11 09:48:40 [INFO] gensim.models.word2vec - PROGRESS: at sentence #160000, processed 2780209 words, keeping 41712 word types\n",
            "2021-03-11 09:48:40 [INFO] gensim.models.word2vec - PROGRESS: at sentence #170000, processed 2952606 words, keeping 42758 word types\n",
            "2021-03-11 09:48:40 [INFO] gensim.models.word2vec - PROGRESS: at sentence #180000, processed 3124225 words, keeping 43730 word types\n",
            "2021-03-11 09:48:40 [INFO] gensim.models.word2vec - PROGRESS: at sentence #190000, processed 3300349 words, keeping 44643 word types\n",
            "2021-03-11 09:48:40 [INFO] gensim.models.word2vec - PROGRESS: at sentence #200000, processed 3474950 words, keeping 45517 word types\n",
            "2021-03-11 09:48:40 [INFO] gensim.models.word2vec - PROGRESS: at sentence #210000, processed 3649746 words, keeping 46505 word types\n",
            "2021-03-11 09:48:40 [INFO] gensim.models.word2vec - PROGRESS: at sentence #220000, processed 3823393 words, keeping 47429 word types\n",
            "2021-03-11 09:48:40 [INFO] gensim.models.word2vec - PROGRESS: at sentence #230000, processed 3997259 words, keeping 48340 word types\n",
            "2021-03-11 09:48:40 [INFO] gensim.models.word2vec - PROGRESS: at sentence #240000, processed 4173473 words, keeping 49238 word types\n",
            "2021-03-11 09:48:40 [INFO] gensim.models.word2vec - PROGRESS: at sentence #250000, processed 4338993 words, keeping 50133 word types\n",
            "2021-03-11 09:48:40 [INFO] gensim.models.word2vec - PROGRESS: at sentence #260000, processed 4512783 words, keeping 50947 word types\n",
            "2021-03-11 09:48:40 [INFO] gensim.models.word2vec - PROGRESS: at sentence #270000, processed 4685525 words, keeping 52009 word types\n",
            "2021-03-11 09:48:40 [INFO] gensim.models.word2vec - PROGRESS: at sentence #280000, processed 4860238 words, keeping 53297 word types\n",
            "2021-03-11 09:48:41 [INFO] gensim.models.word2vec - PROGRESS: at sentence #290000, processed 5035769 words, keeping 54395 word types\n",
            "2021-03-11 09:48:41 [INFO] gensim.models.word2vec - PROGRESS: at sentence #300000, processed 5212605 words, keeping 55411 word types\n",
            "2021-03-11 09:48:41 [INFO] gensim.models.word2vec - PROGRESS: at sentence #310000, processed 5386088 words, keeping 56461 word types\n",
            "2021-03-11 09:48:41 [INFO] gensim.models.word2vec - PROGRESS: at sentence #320000, processed 5563555 words, keeping 57504 word types\n",
            "2021-03-11 09:48:41 [INFO] gensim.models.word2vec - PROGRESS: at sentence #330000, processed 5736521 words, keeping 58498 word types\n",
            "2021-03-11 09:48:41 [INFO] gensim.models.word2vec - PROGRESS: at sentence #340000, processed 5913861 words, keeping 59449 word types\n",
            "2021-03-11 09:48:41 [INFO] gensim.models.word2vec - PROGRESS: at sentence #350000, processed 6087550 words, keeping 60394 word types\n",
            "2021-03-11 09:48:41 [INFO] gensim.models.word2vec - PROGRESS: at sentence #360000, processed 6259268 words, keeping 61247 word types\n",
            "2021-03-11 09:48:41 [INFO] gensim.models.word2vec - PROGRESS: at sentence #370000, processed 6437143 words, keeping 62143 word types\n",
            "2021-03-11 09:48:41 [INFO] gensim.models.word2vec - PROGRESS: at sentence #380000, processed 6612795 words, keeping 63018 word types\n",
            "2021-03-11 09:48:41 [INFO] gensim.models.word2vec - PROGRESS: at sentence #390000, processed 6791309 words, keeping 63801 word types\n",
            "2021-03-11 09:48:41 [INFO] gensim.models.word2vec - PROGRESS: at sentence #400000, processed 6965038 words, keeping 64626 word types\n",
            "2021-03-11 09:48:41 [INFO] gensim.models.word2vec - PROGRESS: at sentence #410000, processed 7137215 words, keeping 65368 word types\n",
            "2021-03-11 09:48:41 [INFO] gensim.models.word2vec - PROGRESS: at sentence #420000, processed 7311498 words, keeping 66214 word types\n",
            "2021-03-11 09:48:41 [INFO] gensim.models.word2vec - PROGRESS: at sentence #430000, processed 7490316 words, keeping 67024 word types\n",
            "2021-03-11 09:48:41 [INFO] gensim.models.word2vec - PROGRESS: at sentence #440000, processed 7664581 words, keeping 67833 word types\n",
            "2021-03-11 09:48:41 [INFO] gensim.models.word2vec - PROGRESS: at sentence #450000, processed 7840566 words, keeping 68652 word types\n",
            "2021-03-11 09:48:41 [INFO] gensim.models.word2vec - PROGRESS: at sentence #460000, processed 8019951 words, keeping 69475 word types\n",
            "2021-03-11 09:48:41 [INFO] gensim.models.word2vec - PROGRESS: at sentence #470000, processed 8196894 words, keeping 70172 word types\n",
            "2021-03-11 09:48:41 [INFO] gensim.models.word2vec - PROGRESS: at sentence #480000, processed 8370836 words, keeping 70935 word types\n",
            "2021-03-11 09:48:41 [INFO] gensim.models.word2vec - PROGRESS: at sentence #490000, processed 8545172 words, keeping 71697 word types\n",
            "2021-03-11 09:48:42 [INFO] gensim.models.word2vec - PROGRESS: at sentence #500000, processed 8718190 words, keeping 72418 word types\n",
            "2021-03-11 09:48:42 [INFO] gensim.models.word2vec - PROGRESS: at sentence #510000, processed 8892150 words, keeping 73149 word types\n",
            "2021-03-11 09:48:42 [INFO] gensim.models.word2vec - PROGRESS: at sentence #520000, processed 9065590 words, keeping 73851 word types\n",
            "2021-03-11 09:48:42 [INFO] gensim.models.word2vec - PROGRESS: at sentence #530000, processed 9242205 words, keeping 74503 word types\n",
            "2021-03-11 09:48:42 [INFO] gensim.models.word2vec - PROGRESS: at sentence #540000, processed 9414643 words, keeping 75182 word types\n",
            "2021-03-11 09:48:42 [INFO] gensim.models.word2vec - PROGRESS: at sentence #550000, processed 9592849 words, keeping 75875 word types\n",
            "2021-03-11 09:48:42 [INFO] gensim.models.word2vec - PROGRESS: at sentence #560000, processed 9766486 words, keeping 76546 word types\n",
            "2021-03-11 09:48:42 [INFO] gensim.models.word2vec - PROGRESS: at sentence #570000, processed 9943016 words, keeping 77197 word types\n",
            "2021-03-11 09:48:42 [INFO] gensim.models.word2vec - PROGRESS: at sentence #580000, processed 10116619 words, keeping 77846 word types\n",
            "2021-03-11 09:48:42 [INFO] gensim.models.word2vec - PROGRESS: at sentence #590000, processed 10291363 words, keeping 78534 word types\n",
            "2021-03-11 09:48:42 [INFO] gensim.models.word2vec - PROGRESS: at sentence #600000, processed 10464065 words, keeping 79119 word types\n",
            "2021-03-11 09:48:42 [INFO] gensim.models.word2vec - PROGRESS: at sentence #610000, processed 10639410 words, keeping 79801 word types\n",
            "2021-03-11 09:48:42 [INFO] gensim.models.word2vec - PROGRESS: at sentence #620000, processed 10815825 words, keeping 80366 word types\n",
            "2021-03-11 09:48:42 [INFO] gensim.models.word2vec - PROGRESS: at sentence #630000, processed 10987254 words, keeping 80982 word types\n",
            "2021-03-11 09:48:42 [INFO] gensim.models.word2vec - PROGRESS: at sentence #640000, processed 11160240 words, keeping 81620 word types\n",
            "2021-03-11 09:48:42 [INFO] gensim.models.word2vec - PROGRESS: at sentence #650000, processed 11336244 words, keeping 82285 word types\n",
            "2021-03-11 09:48:42 [INFO] gensim.models.word2vec - PROGRESS: at sentence #660000, processed 11509490 words, keeping 82845 word types\n",
            "2021-03-11 09:48:42 [INFO] gensim.models.word2vec - PROGRESS: at sentence #670000, processed 11683745 words, keeping 83403 word types\n",
            "2021-03-11 09:48:42 [INFO] gensim.models.word2vec - PROGRESS: at sentence #680000, processed 11859249 words, keeping 83961 word types\n",
            "2021-03-11 09:48:42 [INFO] gensim.models.word2vec - PROGRESS: at sentence #690000, processed 12034144 words, keeping 84599 word types\n",
            "2021-03-11 09:48:42 [INFO] gensim.models.word2vec - PROGRESS: at sentence #700000, processed 12211269 words, keeping 85190 word types\n",
            "2021-03-11 09:48:43 [INFO] gensim.models.word2vec - PROGRESS: at sentence #710000, processed 12384300 words, keeping 85726 word types\n",
            "2021-03-11 09:48:43 [INFO] gensim.models.word2vec - PROGRESS: at sentence #720000, processed 12557978 words, keeping 86268 word types\n",
            "2021-03-11 09:48:43 [INFO] gensim.models.word2vec - PROGRESS: at sentence #730000, processed 12734539 words, keeping 86802 word types\n",
            "2021-03-11 09:48:43 [INFO] gensim.models.word2vec - PROGRESS: at sentence #740000, processed 12907027 words, keeping 87370 word types\n",
            "2021-03-11 09:48:43 [INFO] gensim.models.word2vec - PROGRESS: at sentence #750000, processed 13079780 words, keeping 87886 word types\n",
            "2021-03-11 09:48:43 [INFO] gensim.models.word2vec - PROGRESS: at sentence #760000, processed 13251955 words, keeping 88409 word types\n",
            "2021-03-11 09:48:43 [INFO] gensim.models.word2vec - PROGRESS: at sentence #770000, processed 13429058 words, keeping 88974 word types\n",
            "2021-03-11 09:48:43 [INFO] gensim.models.word2vec - PROGRESS: at sentence #780000, processed 13606539 words, keeping 89491 word types\n",
            "2021-03-11 09:48:43 [INFO] gensim.models.word2vec - PROGRESS: at sentence #790000, processed 13783931 words, keeping 90105 word types\n",
            "2021-03-11 09:48:43 [INFO] gensim.models.word2vec - collected 90266 word types from a corpus of 13831512 raw words and 792761 sentences\n",
            "2021-03-11 09:48:43 [INFO] gensim.models.word2vec - Loading a fresh vocabulary\n",
            "2021-03-11 09:48:43 [INFO] gensim.models.word2vec - effective_min_count=40 retains 12015 unique words (13% of original 90266, drops 78251)\n",
            "2021-03-11 09:48:43 [INFO] gensim.models.word2vec - effective_min_count=40 leaves 13459533 word corpus (97% of original 13831512, drops 371979)\n",
            "2021-03-11 09:48:43 [INFO] gensim.models.word2vec - deleting the raw counts dictionary of 90266 items\n",
            "2021-03-11 09:48:43 [INFO] gensim.models.word2vec - sample=0.001 downsamples 47 most-common words\n",
            "2021-03-11 09:48:43 [INFO] gensim.models.word2vec - downsampling leaves estimated 10817191 word corpus (80.4% of prior 13459533)\n",
            "2021-03-11 09:48:43 [INFO] gensim.models.base_any2vec - estimated required memory for 12015 words and 300 dimensions: 34843500 bytes\n",
            "2021-03-11 09:48:43 [INFO] gensim.models.word2vec - resetting layer weights\n",
            "2021-03-11 09:48:46 [INFO] gensim.models.base_any2vec - training model with 4 workers on 12015 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
            "2021-03-11 09:48:47 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 2.87% examples, 302598 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:48:48 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 5.88% examples, 303365 words/s, in_qsize 7, out_qsize 2\n",
            "2021-03-11 09:48:49 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 9.02% examples, 311755 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:48:50 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 12.01% examples, 313183 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:48:51 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 14.91% examples, 311409 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:48:52 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 17.97% examples, 312435 words/s, in_qsize 6, out_qsize 1\n",
            "2021-03-11 09:48:53 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 20.87% examples, 312383 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:48:54 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 23.86% examples, 311994 words/s, in_qsize 8, out_qsize 1\n",
            "2021-03-11 09:48:55 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 26.83% examples, 312468 words/s, in_qsize 8, out_qsize 2\n",
            "2021-03-11 09:48:56 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 29.91% examples, 314054 words/s, in_qsize 8, out_qsize 1\n",
            "2021-03-11 09:48:57 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 32.94% examples, 314202 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:48:58 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 35.91% examples, 314330 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:48:59 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 38.86% examples, 314359 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:00 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 41.94% examples, 314338 words/s, in_qsize 8, out_qsize 0\n",
            "2021-03-11 09:49:01 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 44.85% examples, 314070 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:02 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 47.77% examples, 314389 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:03 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 50.57% examples, 313822 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:04 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 53.54% examples, 313779 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:05 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 56.47% examples, 313827 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:06 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 59.43% examples, 313570 words/s, in_qsize 6, out_qsize 1\n",
            "2021-03-11 09:49:07 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 62.48% examples, 314108 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:08 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 65.46% examples, 314278 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:09 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 68.42% examples, 313809 words/s, in_qsize 6, out_qsize 1\n",
            "2021-03-11 09:49:10 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 71.56% examples, 314553 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:11 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 74.52% examples, 314680 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:12 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 77.49% examples, 314758 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:13 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 80.40% examples, 314385 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:14 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 83.49% examples, 314756 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:15 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 86.46% examples, 314699 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:16 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 89.41% examples, 314668 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:17 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 92.36% examples, 314705 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:18 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 95.36% examples, 314551 words/s, in_qsize 6, out_qsize 1\n",
            "2021-03-11 09:49:19 [INFO] gensim.models.base_any2vec - EPOCH 1 - PROGRESS: at 98.41% examples, 315146 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:20 [INFO] gensim.models.base_any2vec - worker thread finished; awaiting finish of 3 more threads\n",
            "2021-03-11 09:49:20 [INFO] gensim.models.base_any2vec - worker thread finished; awaiting finish of 2 more threads\n",
            "2021-03-11 09:49:20 [INFO] gensim.models.base_any2vec - worker thread finished; awaiting finish of 1 more threads\n",
            "2021-03-11 09:49:20 [INFO] gensim.models.base_any2vec - worker thread finished; awaiting finish of 0 more threads\n",
            "2021-03-11 09:49:20 [INFO] gensim.models.base_any2vec - EPOCH - 1 : training on 13831512 raw words (10818038 effective words) took 34.3s, 315347 effective words/s\n",
            "2021-03-11 09:49:21 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 2.94% examples, 305840 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:22 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 5.88% examples, 309347 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:23 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 8.80% examples, 308629 words/s, in_qsize 8, out_qsize 3\n",
            "2021-03-11 09:49:24 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 11.86% examples, 312805 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:25 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 14.91% examples, 313932 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:26 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 17.97% examples, 315534 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:27 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 20.89% examples, 313866 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:28 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 24.00% examples, 314808 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:29 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 27.12% examples, 316116 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:30 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 30.20% examples, 316869 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:31 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 33.17% examples, 315414 words/s, in_qsize 8, out_qsize 2\n",
            "2021-03-11 09:49:32 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 36.20% examples, 316177 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:33 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 39.23% examples, 315387 words/s, in_qsize 7, out_qsize 3\n",
            "2021-03-11 09:49:34 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 42.16% examples, 315344 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:35 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 45.05% examples, 314756 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:36 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 48.18% examples, 315144 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:38 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 51.36% examples, 315578 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:39 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 54.30% examples, 315705 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:40 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 57.24% examples, 315881 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:41 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 60.31% examples, 316575 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:42 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 63.14% examples, 316009 words/s, in_qsize 8, out_qsize 1\n",
            "2021-03-11 09:49:43 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 66.01% examples, 315803 words/s, in_qsize 8, out_qsize 1\n",
            "2021-03-11 09:49:44 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 68.98% examples, 315849 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:45 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 71.92% examples, 315605 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:46 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 74.96% examples, 315824 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:47 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 77.85% examples, 315536 words/s, in_qsize 8, out_qsize 0\n",
            "2021-03-11 09:49:48 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 80.83% examples, 315682 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:49 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 83.78% examples, 315758 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:50 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 86.88% examples, 316143 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:51 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 89.82% examples, 315930 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:52 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 92.80% examples, 315866 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:53 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 95.87% examples, 316042 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:54 [INFO] gensim.models.base_any2vec - EPOCH 2 - PROGRESS: at 98.91% examples, 315939 words/s, in_qsize 7, out_qsize 1\n",
            "2021-03-11 09:49:54 [INFO] gensim.models.base_any2vec - worker thread finished; awaiting finish of 3 more threads\n",
            "2021-03-11 09:49:54 [INFO] gensim.models.base_any2vec - worker thread finished; awaiting finish of 2 more threads\n",
            "2021-03-11 09:49:54 [INFO] gensim.models.base_any2vec - worker thread finished; awaiting finish of 1 more threads\n",
            "2021-03-11 09:49:54 [INFO] gensim.models.base_any2vec - worker thread finished; awaiting finish of 0 more threads\n",
            "2021-03-11 09:49:54 [INFO] gensim.models.base_any2vec - EPOCH - 2 : training on 13831512 raw words (10817175 effective words) took 34.2s, 316536 effective words/s\n",
            "2021-03-11 09:49:55 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 2.80% examples, 298259 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:56 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 5.74% examples, 308799 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:57 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 8.81% examples, 310068 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:58 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 11.86% examples, 312954 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:49:59 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 14.77% examples, 311702 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:00 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 17.76% examples, 312882 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:01 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 20.73% examples, 313655 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:02 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 23.64% examples, 312905 words/s, in_qsize 7, out_qsize 1\n",
            "2021-03-11 09:50:03 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 26.60% examples, 313455 words/s, in_qsize 8, out_qsize 1\n",
            "2021-03-11 09:50:04 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 29.55% examples, 313061 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:05 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 32.64% examples, 312702 words/s, in_qsize 7, out_qsize 1\n",
            "2021-03-11 09:50:06 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 35.70% examples, 313219 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:07 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 38.49% examples, 312334 words/s, in_qsize 6, out_qsize 1\n",
            "2021-03-11 09:50:08 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 41.46% examples, 312043 words/s, in_qsize 6, out_qsize 1\n",
            "2021-03-11 09:50:09 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 44.40% examples, 312149 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:11 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 47.41% examples, 311973 words/s, in_qsize 5, out_qsize 2\n",
            "2021-03-11 09:50:12 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 50.35% examples, 312326 words/s, in_qsize 5, out_qsize 2\n",
            "2021-03-11 09:50:13 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 53.40% examples, 312939 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:14 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 56.32% examples, 312683 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:15 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 59.29% examples, 312798 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:16 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 62.27% examples, 312981 words/s, in_qsize 8, out_qsize 0\n",
            "2021-03-11 09:50:17 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 65.23% examples, 313124 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:18 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 68.13% examples, 312895 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:19 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 71.14% examples, 313321 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:20 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 74.22% examples, 313381 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:21 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 77.34% examples, 313502 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:22 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 80.47% examples, 313907 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:23 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 83.49% examples, 314041 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:24 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 86.53% examples, 314229 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:25 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 89.54% examples, 314388 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:26 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 92.66% examples, 314694 words/s, in_qsize 8, out_qsize 0\n",
            "2021-03-11 09:50:27 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 95.58% examples, 314594 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:28 [INFO] gensim.models.base_any2vec - EPOCH 3 - PROGRESS: at 98.62% examples, 315157 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:28 [INFO] gensim.models.base_any2vec - worker thread finished; awaiting finish of 3 more threads\n",
            "2021-03-11 09:50:28 [INFO] gensim.models.base_any2vec - worker thread finished; awaiting finish of 2 more threads\n",
            "2021-03-11 09:50:28 [INFO] gensim.models.base_any2vec - worker thread finished; awaiting finish of 1 more threads\n",
            "2021-03-11 09:50:28 [INFO] gensim.models.base_any2vec - worker thread finished; awaiting finish of 0 more threads\n",
            "2021-03-11 09:50:28 [INFO] gensim.models.base_any2vec - EPOCH - 3 : training on 13831512 raw words (10816619 effective words) took 34.3s, 315347 effective words/s\n",
            "2021-03-11 09:50:29 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 2.80% examples, 297229 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:31 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 5.80% examples, 304246 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:32 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 8.81% examples, 308744 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:33 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 11.87% examples, 310845 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:34 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 14.99% examples, 312014 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:35 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 18.04% examples, 312601 words/s, in_qsize 8, out_qsize 1\n",
            "2021-03-11 09:50:36 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 21.25% examples, 313804 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:37 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 24.15% examples, 313319 words/s, in_qsize 6, out_qsize 1\n",
            "2021-03-11 09:50:38 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 27.27% examples, 313920 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:39 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 30.28% examples, 314894 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:40 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 33.31% examples, 314520 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:41 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 36.34% examples, 314313 words/s, in_qsize 5, out_qsize 2\n",
            "2021-03-11 09:50:42 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 39.37% examples, 314962 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:43 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 42.29% examples, 315187 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:44 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 45.42% examples, 315122 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:45 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 48.39% examples, 315675 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:46 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 51.44% examples, 315652 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:47 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 54.30% examples, 314836 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:48 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 57.31% examples, 315200 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:49 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 60.31% examples, 315793 words/s, in_qsize 8, out_qsize 0\n",
            "2021-03-11 09:50:50 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 63.28% examples, 315936 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:51 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 66.16% examples, 315251 words/s, in_qsize 4, out_qsize 3\n",
            "2021-03-11 09:50:52 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 69.26% examples, 315869 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:53 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 72.13% examples, 315709 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:54 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 75.10% examples, 315880 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:55 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 78.06% examples, 315833 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:56 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 81.04% examples, 315974 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:57 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 84.00% examples, 315955 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:50:58 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 86.88% examples, 315422 words/s, in_qsize 8, out_qsize 1\n",
            "2021-03-11 09:50:59 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 89.97% examples, 316090 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:51:00 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 92.87% examples, 315896 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:51:01 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 95.87% examples, 315598 words/s, in_qsize 6, out_qsize 1\n",
            "2021-03-11 09:51:02 [INFO] gensim.models.base_any2vec - EPOCH 4 - PROGRESS: at 98.83% examples, 315656 words/s, in_qsize 7, out_qsize 1\n",
            "2021-03-11 09:51:03 [INFO] gensim.models.base_any2vec - worker thread finished; awaiting finish of 3 more threads\n",
            "2021-03-11 09:51:03 [INFO] gensim.models.base_any2vec - worker thread finished; awaiting finish of 2 more threads\n",
            "2021-03-11 09:51:03 [INFO] gensim.models.base_any2vec - worker thread finished; awaiting finish of 1 more threads\n",
            "2021-03-11 09:51:03 [INFO] gensim.models.base_any2vec - worker thread finished; awaiting finish of 0 more threads\n",
            "2021-03-11 09:51:03 [INFO] gensim.models.base_any2vec - EPOCH - 4 : training on 13831512 raw words (10817530 effective words) took 34.2s, 316132 effective words/s\n",
            "2021-03-11 09:51:04 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 2.80% examples, 289481 words/s, in_qsize 8, out_qsize 1\n",
            "2021-03-11 09:51:05 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 5.81% examples, 302102 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:51:06 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 8.81% examples, 307520 words/s, in_qsize 6, out_qsize 1\n",
            "2021-03-11 09:51:07 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 11.86% examples, 308480 words/s, in_qsize 5, out_qsize 2\n",
            "2021-03-11 09:51:08 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 14.92% examples, 309865 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:51:09 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 17.97% examples, 312648 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:51:10 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 20.81% examples, 311207 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:51:11 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 23.85% examples, 311694 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:51:12 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 26.90% examples, 311774 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:51:13 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 29.98% examples, 313455 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:51:14 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 33.02% examples, 313921 words/s, in_qsize 8, out_qsize 0\n",
            "2021-03-11 09:51:15 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 35.98% examples, 313606 words/s, in_qsize 8, out_qsize 1\n",
            "2021-03-11 09:51:16 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 38.92% examples, 313607 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:51:17 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 41.94% examples, 314111 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:51:18 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 44.91% examples, 313338 words/s, in_qsize 4, out_qsize 3\n",
            "2021-03-11 09:51:19 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 47.98% examples, 314370 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:51:20 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 50.92% examples, 314468 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:51:21 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 53.82% examples, 314171 words/s, in_qsize 4, out_qsize 3\n",
            "2021-03-11 09:51:22 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 56.83% examples, 314597 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:51:23 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 59.79% examples, 314526 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:51:24 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 62.78% examples, 314486 words/s, in_qsize 7, out_qsize 1\n",
            "2021-03-11 09:51:25 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 65.81% examples, 315030 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:51:26 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 68.91% examples, 315237 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:51:27 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 71.77% examples, 314924 words/s, in_qsize 7, out_qsize 1\n",
            "2021-03-11 09:51:28 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 74.74% examples, 314906 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:51:29 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 77.70% examples, 314984 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:51:30 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 80.54% examples, 314536 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:51:31 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 83.49% examples, 314380 words/s, in_qsize 8, out_qsize 0\n",
            "2021-03-11 09:51:32 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 86.31% examples, 313964 words/s, in_qsize 5, out_qsize 2\n",
            "2021-03-11 09:51:33 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 89.20% examples, 313796 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:51:34 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 92.13% examples, 313334 words/s, in_qsize 8, out_qsize 2\n",
            "2021-03-11 09:51:35 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 95.20% examples, 313608 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:51:37 [INFO] gensim.models.base_any2vec - EPOCH 5 - PROGRESS: at 98.21% examples, 313647 words/s, in_qsize 7, out_qsize 0\n",
            "2021-03-11 09:51:37 [INFO] gensim.models.base_any2vec - worker thread finished; awaiting finish of 3 more threads\n",
            "2021-03-11 09:51:37 [INFO] gensim.models.base_any2vec - worker thread finished; awaiting finish of 2 more threads\n",
            "2021-03-11 09:51:37 [INFO] gensim.models.base_any2vec - worker thread finished; awaiting finish of 1 more threads\n",
            "2021-03-11 09:51:37 [INFO] gensim.models.base_any2vec - worker thread finished; awaiting finish of 0 more threads\n",
            "2021-03-11 09:51:37 [INFO] gensim.models.base_any2vec - EPOCH - 5 : training on 13831512 raw words (10816637 effective words) took 34.4s, 314202 effective words/s\n",
            "2021-03-11 09:51:37 [INFO] gensim.models.base_any2vec - training on a 69157560 raw words (54085999 effective words) took 171.5s, 315403 effective words/s\n",
            "2021-03-11 09:51:37 [INFO] gensim.models.keyedvectors - precomputing L2-norms of word weight vectors\n",
            "2021-03-11 09:51:37 [INFO] gensim.utils - saving Word2Vec object under 300features_40minwords_10context, separately None\n",
            "2021-03-11 09:51:37 [INFO] gensim.utils - not storing attribute vectors_norm\n",
            "2021-03-11 09:51:37 [INFO] gensim.utils - not storing attribute cum_table\n",
            "2021-03-11 09:51:38 [INFO] gensim.utils - saved 300features_40minwords_10context\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAzZkVCWubxD",
        "outputId": "82e966af-c2e2-4c0a-fe74-a814058161a4"
      },
      "source": [
        "# save model to drive for later use OPTIONAL\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "model_save_name = f'Word2Vec({num_features},{min_word_count},{context})'\n",
        "\n",
        "#!ls /content/drive/MyDrive\n",
        "\n",
        "path = f\"/content/drive/MyDrive/{model_save_name}\" \n",
        "model.save(path)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "'Assistive Tech for Mental Health'\n",
            " cleaned_processed.cleveland.data\n",
            "'Colab Notebooks'\n",
            "'Copy of ideation-assessment-group5.gsheet'\n",
            "'Copy of unconstrained-design-design-groupX.gslides'\n",
            "'CSC370 AS1.drawio'\n",
            " CSC474A1_neural_networks.ipynb\n",
            " CSC474A1_random_forest.ipynb\n",
            "'CSC485D Visual Interim Presentation.gslides'\n",
            "'Data Mining Lab 1'\n",
            "'Data Mining Lab 1 - Jan 22nd by Keon  (1).ipynb'\n",
            "'Lab2 MLP.ipynb'\n",
            "'Lab4 - Logistic Regression .ipynb'\n",
            "'Learning Roadmap.drawio'\n",
            "'Project Evaluation (8% of total 40% project grade, due 7pm Sunday March 15th by email).gdoc'\n",
            "'Status report.gslides'\n",
            "'Untitled Diagram (1).drawio'\n",
            "'Untitled Diagram.drawio'\n",
            "2021-03-11 10:15:53 [INFO] gensim.utils - saving Word2Vec object under /content/drive/MyDrive/Word2Vec(300,40,10), separately None\n",
            "2021-03-11 10:15:53 [INFO] gensim.utils - not storing attribute vectors_norm\n",
            "2021-03-11 10:15:53 [INFO] gensim.utils - not storing attribute cum_table\n",
            "2021-03-11 10:15:54 [INFO] gensim.utils - saved /content/drive/MyDrive/Word2Vec(300,40,10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghysJ0p6y2Qv"
      },
      "source": [
        "load trained Word2Vec model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mAs-vbdyizc"
      },
      "source": [
        "# load trained model OPTIONAL\n",
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec.load(\"Word2Vec(300,40,10)\")\n",
        "model.trainables.syn1neg.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzsT36mdp5To",
        "outputId": "03d4e891-eb22-4558-a6bb-5915d60f7e66"
      },
      "source": [
        "model.most_similar(\"woman\")"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ladi', 0.6253357529640198),\n",
              " ('girl', 0.5801959037780762),\n",
              " ('man', 0.5739419460296631),\n",
              " ('widow', 0.566925048828125),\n",
              " ('prostitut', 0.5554071664810181),\n",
              " ('women', 0.5501986145973206),\n",
              " ('her', 0.5369620323181152),\n",
              " ('daughter', 0.5232342481613159),\n",
              " ('housewif', 0.5221171379089355),\n",
              " ('waitress', 0.5202633142471313)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jq-zQbNNza8J",
        "outputId": "e016ff8f-1f4b-447e-d3d1-25958dec485e"
      },
      "source": [
        "model.wv[\"man\"] # word vec"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.06165772,  0.02414589,  0.0058367 , -0.00124058,  0.08935915,\n",
              "        0.07299625, -0.04081979,  0.08348368,  0.06279206,  0.0464688 ,\n",
              "        0.04704444, -0.00869097,  0.04606373, -0.08394785,  0.01960148,\n",
              "       -0.05242679, -0.01590137, -0.04255367,  0.0136599 ,  0.03912215,\n",
              "        0.07074215, -0.02785238, -0.01252544, -0.0279937 , -0.07397655,\n",
              "       -0.06079627, -0.07359461, -0.08928838,  0.03222402,  0.00256313,\n",
              "        0.01830097, -0.04056092,  0.0269219 ,  0.02067096, -0.13578847,\n",
              "        0.04455758, -0.01085038,  0.04109224, -0.08153564,  0.02400051,\n",
              "       -0.06374152,  0.08122293, -0.04296341, -0.08774059, -0.01360485,\n",
              "        0.00888975,  0.00342898,  0.01010613,  0.01431305,  0.02722558,\n",
              "       -0.02642804,  0.05952154,  0.00172234,  0.07008486,  0.12017436,\n",
              "       -0.10503765,  0.01424578, -0.07151203,  0.02760548, -0.02071025,\n",
              "       -0.02047265,  0.00166217,  0.02733372,  0.02464361, -0.01315925,\n",
              "        0.01201706, -0.06160785, -0.03325102,  0.11356603,  0.03692517,\n",
              "        0.1099593 ,  0.04373652, -0.01759732, -0.05561911, -0.14053322,\n",
              "        0.07347547,  0.01633257,  0.03738718,  0.03229051, -0.05662895,\n",
              "       -0.0942551 ,  0.0513487 ,  0.04422083,  0.01266895,  0.03456761,\n",
              "       -0.00451437,  0.0251522 , -0.04047617, -0.04078007,  0.04714487,\n",
              "       -0.03529755,  0.02768456,  0.01812385, -0.08227083, -0.04468865,\n",
              "       -0.04448427, -0.01248666, -0.02601206,  0.02364256, -0.07588374,\n",
              "       -0.02792291, -0.03114181,  0.05853096,  0.11868556,  0.01392918,\n",
              "       -0.05116509,  0.03336963,  0.0656715 ,  0.01063353, -0.08269266,\n",
              "        0.05189705, -0.07958637, -0.05619869,  0.00202984,  0.09704632,\n",
              "        0.19084929, -0.01942198, -0.00284696,  0.02399201,  0.00259445,\n",
              "        0.02502898, -0.03521875, -0.04638568, -0.06476081,  0.05268782,\n",
              "        0.01553437,  0.06478591, -0.02561533, -0.05838525,  0.01796194,\n",
              "       -0.07243483, -0.0299285 ,  0.00538881, -0.07124998, -0.04412761,\n",
              "        0.01761364, -0.07787971, -0.00123894,  0.03331936, -0.04893143,\n",
              "       -0.04234558,  0.1512991 , -0.00253206,  0.03215078, -0.16930644,\n",
              "       -0.1203369 , -0.0415625 , -0.017165  ,  0.01652229,  0.01005999,\n",
              "       -0.01785321,  0.00044534,  0.07110069, -0.10052094,  0.01986726,\n",
              "        0.05427738,  0.00272769,  0.01752982, -0.05720197, -0.02628708,\n",
              "       -0.06525455,  0.08412319, -0.00388982, -0.06590313, -0.03874238,\n",
              "       -0.13999578,  0.00594377, -0.04632352, -0.08497722,  0.07529607,\n",
              "        0.02074899,  0.00959261,  0.18779126, -0.07411953,  0.0078083 ,\n",
              "        0.09390892,  0.02744051,  0.02544999,  0.09698415,  0.04416473,\n",
              "        0.01740414, -0.06810278,  0.03816897,  0.11855451,  0.02075358,\n",
              "        0.05099781, -0.06802301, -0.00919444, -0.12795898,  0.01355833,\n",
              "       -0.01395389, -0.12178849,  0.02070238, -0.05962823,  0.01287842,\n",
              "       -0.04422941, -0.04308464, -0.01371052,  0.02081259, -0.07310792,\n",
              "        0.05631795,  0.09990997, -0.05199993, -0.08865943, -0.08376174,\n",
              "        0.01272665, -0.03952681,  0.04989119, -0.00366953, -0.00594342,\n",
              "        0.06516425,  0.10274324, -0.05458498,  0.0931702 , -0.05315425,\n",
              "       -0.07777005,  0.03213893, -0.10445905, -0.03408235, -0.02165674,\n",
              "       -0.01164123, -0.08363456,  0.02725449,  0.01430557, -0.0436582 ,\n",
              "        0.00057215, -0.0119529 , -0.06813756,  0.09136486,  0.00473818,\n",
              "        0.0791075 ,  0.05386943, -0.01246812,  0.06326395, -0.00721459,\n",
              "       -0.05640867, -0.01482001,  0.0779274 , -0.01765342,  0.09851345,\n",
              "       -0.02658071, -0.06664009, -0.04468689,  0.00823945,  0.02414785,\n",
              "        0.12841853, -0.06189477, -0.00042404, -0.0037617 ,  0.03997231,\n",
              "       -0.05339521, -0.03653088, -0.00895693, -0.08787184, -0.03427773,\n",
              "        0.00415724,  0.01212447,  0.05676062, -0.1104745 ,  0.08236629,\n",
              "       -0.01885598,  0.14639606,  0.00566192, -0.02033125, -0.03693908,\n",
              "        0.0181456 , -0.00363019,  0.08464336,  0.01982862,  0.01184221,\n",
              "       -0.00861717, -0.01223481, -0.03808377,  0.07944074,  0.04385152,\n",
              "        0.02427638,  0.01896702,  0.03291373, -0.00725361, -0.01361203,\n",
              "        0.00804132, -0.04564845, -0.05986588, -0.02496538, -0.014817  ,\n",
              "        0.03241305,  0.0916263 ,  0.06404627, -0.03382701,  0.0797715 ,\n",
              "       -0.06361531,  0.02515819,  0.06798512,  0.06411552,  0.03574898,\n",
              "        0.02462944,  0.03215724,  0.02072633,  0.00293473, -0.00470092],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzcF6VHr3Ez5"
      },
      "source": [
        "# Build Feature Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvJfxeve36Le"
      },
      "source": [
        "get the feature set by averaging the word vectors in a single review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbipgPWQ24_A"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# take a list of words as input, return average vector\n",
        "def get_average_vec(review,  n_features = num_features):\n",
        "    vectorized = [model.wv[word] for word in review if word in model.wv.vocab]\n",
        "    total = len(vectorized)\n",
        "    sum_v = np.sum(vectorized, axis=0)\n",
        "    average_v = np.divide(sum_v, total)\n",
        "    return average_v"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q7912PWIHza"
      },
      "source": [
        "Same preprocessing as word2vec to keep data uniform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_U-q_1x52Gc",
        "outputId": "6840f368-769d-48c0-8da7-5d4030d22cd9"
      },
      "source": [
        "clean_train_reviews = []  \n",
        "\n",
        "print(f'Processing {len(train_data[\"review\"])} training reviews...')\n",
        "train_size = len(train_data[\"review\"])\n",
        "for i in range (0, train_size):\n",
        "    # report progress\n",
        "    progress = (i+1)/train_size *100\n",
        "    if( progress%20 == 0 ):\n",
        "        print(f'   {progress}%')  \n",
        "    avg_v = get_average_vec(clean_sentence(train_data[\"review\"][i]))\n",
        "    clean_train_reviews.append(avg_v)\n",
        "\n",
        "'''\n",
        "clean_test_reviews = [] \n",
        "\n",
        "print(f'Processing {len(test_data[\"review\"])} testing reviews...')\n",
        "test_size = len(test_data[\"review\"])\n",
        "for i in range (0, test_size):\n",
        "    # report progress\n",
        "    progress = (i+1)/test_size *100\n",
        "    if( progress%20 == 0 ):\n",
        "        print(f'   {progress}%')  \n",
        "    avg_v = get_average_vec(clean_sentence(test_data[\"review\"][i]))\n",
        "    clean_test_reviews.append(avg_v)\n",
        "'''"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing 25000 training reviews...\n",
            "   20.0%\n",
            "   40.0%\n",
            "   60.0%\n",
            "   80.0%\n",
            "   100.0%\n",
            "Processing 25000 testing reviews...\n",
            "   20.0%\n",
            "   40.0%\n",
            "   60.0%\n",
            "   80.0%\n",
            "   100.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvYIjkHgDVBK"
      },
      "source": [
        "# Classifier Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsolHlbcJBkk"
      },
      "source": [
        "## Random forest\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v_3se6kG0tf"
      },
      "source": [
        "# splitting train test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(clean_train_reviews, train_data[\"sentiment\"], test_size=0.2, random_state=42)"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9jw8VXYDEPs",
        "outputId": "03bc1e74-8020-43a8-b3c4-96e78b4d37bb"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier as rfc\n",
        "\n",
        "RF = rfc(n_estimators=100)\n",
        "\n",
        "# train\n",
        "RF = RF.fit(X_train, y_train)\n",
        "\n",
        "print(\"Test accuracy:\" ,RF.score(X_test, y_test))\n",
        "\n",
        "# predict\n",
        "#result = RF.predict(y_train)\n"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.8316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CSmWEaBJUGC"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuF9LzPCJYGZ"
      },
      "source": [
        "## Bayes"
      ]
    }
  ]
}